<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 4641: Project Proposal</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 20px;
            background-color: #f4f4f4;
            border-radius: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        h1 {
            text-align: center;
        }
        ol {
            margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>CS 4641: Project Proposal</h1>
<h2>Group 44</h2>

<h3>Introduction and Background</h3>
<p>The project aims to develop a machine learning (ML) model that is capable of automatically categorizing and sorting photos based on individual faces.</p>

<h3>Literature Review</h3>
<p>Face Mesh accurately identifies facial features by mapping key points, and enhancing recognition. Sobel’s Gray-Scale method aids edge detection, focusing on significant facial areas while reducing noise. Background removal isolates the face, ensuring the model concentrates on relevant features. Breaking down face scans into images allows effective data categorization for training datasets. SVM classifies extracted features, while GNN improves accuracy by measuring distances between facial features. InsightFace can provide insights for model development. Using CNN to identify numbers from images, they got 97% accuracy.</p>

<h3>Data Collection and Dataset Description</h3>
<p>To avoid infringing privacy and to simplify data collection efforts, this project will only use images of the team members. The images will be collected by extracting individual frames from recorded videos of each team member’s face using a Python script [4]. These images will be organized into test and training data with team members’ names as labels.</p>
<p>As the project progresses, and given time, we plan to expand the dataset and enhance the models’ recognition capabilities to accommodate multiple individuals in each image.</p>

<h3>Dataset Link</h3>
<p><a href="https://drive.google.com/drive/folders/1mbv-Qf2SaQAVglhvrNN7fcOf6N6mnxUr?usp=sharing">Dataset Link</a></p>

<h3>Problem Definition & Motivation</h3>
<p>At large events like weddings and conferences, sorting through numerous photos can be time-consuming. To address this inefficiency, we aim to develop an ML solution that automates the categorization of images based on facial recognition. Users will submit a video of their face, and our model will extract all photos in which they appear.</p>

<h3>Methods</h3>
<h4>Data Pre-Processing:</h4>
<ul>
    <li>Face Mesh: Captures facial features and gives a geometric description of the face [1].</li>
    <li>Grey Scale: Removes color and noise from the image, making it easier to focus on the target [2].</li>
    <li>Background Filtering: Removes noise from the background and focuses on the target alone [3].</li>
    <li>Frame Extraction: Breaks a video down to multiple photos from different angles [4].</li>
</ul>

<h4>Supervised ML Model:</h4>
<ul>
    <li>Convolutional Neural Network: Used for image recognition tasks [5].</li>
    <li>Graph Neural Network: Used with graph-structured data to model spatial dependencies and relationships between facial landmarks [6].</li>
    <li>Support Vector Machine: A classification algorithm that finds a hyperplane to separate data into classes with the maximum margin [4].</li>
</ul>

<h3>Results and Discussion</h3>
<h4>Quantitative Metrics</h4>
<ul>
    <li>Accuracy: Measured as the ratio of images correctly labeled by our model [7].</li>
    <li>Precision: Measured as the ratio of True Positives to the sum of True Positives and False Positives [7].</li>
    <li>Recall: Measured as the ratio of True Positives to the sum of True Positives and False Negatives [7].</li>
    <li>F1-Score: Measured as the harmonic mean between Recall and Precision [7].</li>
</ul>

<h3>Project Goals</h3>
<ul>
    <li>Achieve at least .7 in Accuracy, Precision, and Recall.</li>
    <li>Ensure low computational intensity to ensure operability on older machines.</li>
    <li>Protect user privacy by restricting access to images where they are identified or cannot be confidently classified (dependent on model Accuracy).</li>
</ul>

<h3>Expected Results</h3>
<ul>
    <li>Attain an Accuracy, Precision, and Recall of at least 0.7 with an F1-Score of at least 0.73.</li>
    <li>Image processing time under 2.5 seconds.</li>
</ul>

<h3>References</h3>
<ol>
    <li>MediaPipe, “MediaPipe Python Tasks - Face Landmarker,” Available: <a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a>. [Accessed: 04-Oct-2024].</li>
    <li>L. Haidar, “Edge Detection Techniques: Sobel vs. Canny,” GitHub, 2024. Available: <a href="https://github.com/lina-haidar/Edge-Detection-Techniques-Sobel-vs.-Canny">https://github.com/lina-haidar/Edge-Detection-Techniques-Sobel-vs.-Canny</a>. [Accessed: 04-Oct-2024].</li>
    <li>G. Shperber, “Background Removal with Deep Learning,” Medium, 2024. Available: <a href="https://towardsdatascience.com/background-removal-with-deep-learning-c4f2104b3157">https://towardsdatascience.com/background-removal-with-deep-learning-c4f2104b3157</a>. [Accessed: 04-Oct-2024].</li>
    <li>D. Sadek, “Advanced Face Recognition System,” Medium, 2024. Available: <a href="https://medium.com/the-modern-scientist/advanced-face-recognition-system-a392787cfe6c">https://medium.com/the-modern-scientist/advanced-face-recognition-system-a392787cfe6c</a>. [Accessed: 04-Oct-2024].</li>
    <li>DeepInsight, “InsightFace: CNN-based Face Recognition,” GitHub, 2024. Available: <a href="https://github.com/deepinsight/insightface">https://github.com/deepinsight/insightface</a>. [Accessed: 04-Oct-2024].</li>
    <li>DataCamp, “A Comprehensive Introduction to Graph Neural Networks (GNNs),” DataCamp, 2024. Available: <a href="https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial">https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial</a>. [Accessed: 04-Oct-2024].</li>
    <li>Scikit-learn, “Model Evaluation: Metrics,” Scikit-learn, 2024. Available: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>. [Accessed: 04-Oct-2024].</li>
</ol>

<h3>Contribution Table</h3>
<table border="1" cellspacing="0" cellpadding="5">
    <tr>
        <th>Name</th>
        <th>Proposal Contributions</th>
    </tr>
    <tr>
        <td>Aakash Asthana</td>
        <td>Potential Results & Discussion, Presentation, Video</td>
    </tr>
    <tr>
        <td>Akshara Joshipura</td>
        <td>Introduction & Background, Gantt Chart, Contribution Table, Proofreading/Editing, Potential Results & Discussion</td>
    </tr>
    <tr>
        <td>Manush Patel</td>
        <td>Machine Learning Models & Preprocessing Approaches, Literature Review, Presentation & Video</td>
    </tr>
    <tr>
        <td>Sami Moussa</td>
        <td>Machine Learning Models & Preprocessing Approaches, Literature Review, Presentation, GitHub, Video</td>
    </tr>
    <tr>
        <td>Shan Patel</td>
        <td>Introduction & Background, Problem Definition, Reference Formatting, Presentation, Video, GitHub</td
